{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation & Error Analysis\n",
    "\n",
    "This notebook performs comprehensive evaluation of the trained invoice extraction models.\n",
    "\n",
    "## Evaluation Framework\n",
    "- **Precision, Recall, F1-score** per entity and overall\n",
    "- **Error Analysis** with false positives/negatives identification\n",
    "- **Cross-validation** results analysis\n",
    "- **Post-processing** validation and normalization\n",
    "- **Quality metrics** for production readiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('..')\n",
    "from extraction_service import InvoiceExtractor\n",
    "\n",
    "print(\"‚úì Evaluation notebook initialized\")\n",
    "print(f\"‚úì Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Training Results & Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training results\n",
    "with open('../models/training_summary.json', 'r', encoding='utf-8') as f:\n",
    "    training_summary = json.load(f)\n",
    "\n",
    "with open('../models/training_history.json', 'r', encoding='utf-8') as f:\n",
    "    training_history = json.load(f)\n",
    "\n",
    "# Load models and extractors\n",
    "best_model_name = training_summary['best_model']['name'].lower().replace(' ', '_')\n",
    "\n",
    "with open(f'../models/best_model_{best_model_name}.pkl', 'rb') as f:\n",
    "    best_model = pickle.load(f)\n",
    "\n",
    "with open('../models/feature_extractor.pkl', 'rb') as f:\n",
    "    feature_extractor = pickle.load(f)\n",
    "\n",
    "with open('../models/tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded training results and models\")\n",
    "print(f\"   - Best model: {training_summary['best_model']['name']}\")\n",
    "print(f\"   - Best F1 score: {training_summary['best_model']['f1_score']:.4f}\")\n",
    "print(f\"   - Total iterations: {training_summary['total_iterations']}\")\n",
    "print(f\"   - Target achieved: {training_summary['target_achieved']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cross-Validation Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossValidationEvaluator:\n",
    "    \"\"\"Perform cross-validation evaluation on all k-folds\"\"\"\n",
    "    \n",
    "    def __init__(self, k_folds: int = 5):\n",
    "        self.k_folds = k_folds\n",
    "        self.cv_results = []\n",
    "    \n",
    "    def evaluate_all_folds(self):\n",
    "        \"\"\"Evaluate model on all k-fold splits\"\"\"\n",
    "        print(f\"üîÑ Evaluating model on {self.k_folds} cross-validation folds...\")\n",
    "        \n",
    "        fold_f1_scores = []\n",
    "        fold_results = []\n",
    "        \n",
    "        for fold in range(1, self.k_folds + 1):\n",
    "            try:\n",
    "                # Load fold data\n",
    "                with open(f'../data/fold_{fold}_train.json', 'r', encoding='utf-8') as f:\n",
    "                    train_data = json.load(f)\n",
    "                \n",
    "                with open(f'../data/fold_{fold}_val.json', 'r', encoding='utf-8') as f:\n",
    "                    val_data = json.load(f)\n",
    "                \n",
    "                # Prepare data\n",
    "                X_train_tokens, y_train_labels = tokenizer.preprocess_for_training(train_data)\n",
    "                X_val_tokens, y_val_labels = tokenizer.preprocess_for_training(val_data)\n",
    "                \n",
    "                X_train_features, y_train_encoded = feature_extractor.prepare_training_data(X_train_tokens, y_train_labels)\n",
    "                X_val_features, y_val_encoded = feature_extractor.prepare_training_data(X_val_tokens, y_val_labels)\n",
    "                \n",
    "                # Train model on this fold\n",
    "                fold_model = type(best_model)(**best_model.get_params())\n",
    "                fold_model.fit(X_train_features, y_train_encoded)\n",
    "                \n",
    "                # Evaluate\n",
    "                y_pred = fold_model.predict(X_val_features)\n",
    "                fold_f1 = f1_score(y_val_encoded, y_pred, average='weighted')\n",
    "                \n",
    "                # Detailed metrics\n",
    "                report = classification_report(y_val_encoded, y_pred, \n",
    "                                             target_names=feature_extractor.label_encoder.classes_,\n",
    "                                             output_dict=True, zero_division=0)\n",
    "                \n",
    "                fold_result = {\n",
    "                    'fold': fold,\n",
    "                    'f1_score': fold_f1,\n",
    "                    'train_size': len(train_data),\n",
    "                    'val_size': len(val_data),\n",
    "                    'classification_report': report\n",
    "                }\n",
    "                \n",
    "                fold_results.append(fold_result)\n",
    "                fold_f1_scores.append(fold_f1)\n",
    "                \n",
    "                print(f\"   Fold {fold}: F1 = {fold_f1:.4f} (Train: {len(train_data)}, Val: {len(val_data)})\")\n",
    "                \n",
    "            except FileNotFoundError:\n",
    "                print(f\"   ‚ö†Ô∏è  Fold {fold} data not found, skipping...\")\n",
    "                continue\n",
    "        \n",
    "        if fold_f1_scores:\n",
    "            self.cv_results = fold_results\n",
    "            \n",
    "            # Summary statistics\n",
    "            mean_f1 = np.mean(fold_f1_scores)\n",
    "            std_f1 = np.std(fold_f1_scores)\n",
    "            \n",
    "            print(f\"\\nüìä Cross-Validation Results:\")\n",
    "            print(f\"   - Mean F1 Score: {mean_f1:.4f} ¬± {std_f1:.4f}\")\n",
    "            print(f\"   - Best Fold F1: {max(fold_f1_scores):.4f}\")\n",
    "            print(f\"   - Worst Fold F1: {min(fold_f1_scores):.4f}\")\n",
    "            print(f\"   - Coefficient of Variation: {(std_f1/mean_f1)*100:.2f}%\")\n",
    "            \n",
    "            return {\n",
    "                'mean_f1': mean_f1,\n",
    "                'std_f1': std_f1,\n",
    "                'fold_scores': fold_f1_scores,\n",
    "                'cv_stable': std_f1 < 0.05  # CV is stable if std < 0.05\n",
    "            }\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def plot_cv_results(self, cv_summary):\n",
    "        \"\"\"Plot cross-validation results\"\"\"\n",
    "        if not cv_summary:\n",
    "            print(\"No CV results to plot\")\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # F1 scores by fold\n",
    "        plt.subplot(2, 2, 1)\n",
    "        folds = range(1, len(cv_summary['fold_scores']) + 1)\n",
    "        plt.bar(folds, cv_summary['fold_scores'], alpha=0.7, color='skyblue')\n",
    "        plt.axhline(y=cv_summary['mean_f1'], color='red', linestyle='--', label=f'Mean: {cv_summary[\"mean_f1\"]:.3f}')\n",
    "        plt.axhline(y=0.9, color='green', linestyle='--', label='Target: 0.9')\n",
    "        plt.xlabel('Fold')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title('F1 Score by Cross-Validation Fold')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Box plot of F1 scores\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.boxplot(cv_summary['fold_scores'])\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title('F1 Score Distribution Across Folds')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Per-entity performance (from best fold)\n",
    "        if self.cv_results:\n",
    "            best_fold_idx = np.argmax(cv_summary['fold_scores'])\n",
    "            best_fold_report = self.cv_results[best_fold_idx]['classification_report']\n",
    "            \n",
    "            # Extract per-class F1 scores\n",
    "            entity_f1 = {}\n",
    "            for class_name, metrics in best_fold_report.items():\n",
    "                if isinstance(metrics, dict) and 'f1-score' in metrics:\n",
    "                    if class_name not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "                        entity_f1[class_name] = metrics['f1-score']\n",
    "            \n",
    "            if entity_f1:\n",
    "                plt.subplot(2, 2, 3)\n",
    "                entities = list(entity_f1.keys())[:10]  # Top 10 entities\n",
    "                scores = [entity_f1[e] for e in entities]\n",
    "                \n",
    "                plt.barh(entities, scores, alpha=0.7)\n",
    "                plt.axvline(x=0.9, color='red', linestyle='--', label='Target: 0.9')\n",
    "                plt.xlabel('F1 Score')\n",
    "                plt.title('Per-Entity F1 Scores (Best Fold)')\n",
    "                plt.legend()\n",
    "                plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Stability analysis\n",
    "        plt.subplot(2, 2, 4)\n",
    "        stability_metrics = {\n",
    "            'Mean F1': cv_summary['mean_f1'],\n",
    "            'Std Dev': cv_summary['std_f1'],\n",
    "            'Min F1': min(cv_summary['fold_scores']),\n",
    "            'Max F1': max(cv_summary['fold_scores'])\n",
    "        }\n",
    "        \n",
    "        metrics = list(stability_metrics.keys())\n",
    "        values = list(stability_metrics.values())\n",
    "        colors = ['blue', 'orange', 'red', 'green']\n",
    "        \n",
    "        plt.bar(metrics, values, color=colors, alpha=0.7)\n",
    "        plt.title('Cross-Validation Stability Metrics')\n",
    "        plt.ylabel('Score')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        for i, v in enumerate(values):\n",
    "            plt.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Run cross-validation evaluation\n",
    "cv_evaluator = CrossValidationEvaluator(k_folds=5)\n",
    "cv_summary = cv_evaluator.evaluate_all_folds()\n",
    "\n",
    "if cv_summary:\n",
    "    cv_evaluator.plot_cv_results(cv_summary)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Could not perform cross-validation evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Error Analysis & False Positives/Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorAnalyzer:\n",
    "    \"\"\"Analyze model errors and identify improvement opportunities\"\"\"\n",
    "    \n",
    "    def __init__(self, model, feature_extractor, tokenizer):\n",
    "        self.model = model\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.error_analysis = {}\n",
    "    \n",
    "    def analyze_predictions(self, test_data: List[Dict], sample_size: int = 50):\n",
    "        \"\"\"Analyze model predictions and identify errors\"\"\"\n",
    "        print(f\"üîç Analyzing model predictions on {min(sample_size, len(test_data))} samples...\")\n",
    "        \n",
    "        # Prepare test data\n",
    "        X_tokens, y_labels = self.tokenizer.preprocess_for_training(test_data[:sample_size])\n",
    "        X_features, y_encoded = self.feature_extractor.prepare_training_data(X_tokens, y_labels)\n",
    "        \n",
    "        # Get predictions\n",
    "        y_pred = self.model.predict(X_features)\n",
    "        \n",
    "        # Convert back to label names\n",
    "        y_true_labels = self.feature_extractor.label_encoder.inverse_transform(y_encoded)\n",
    "        y_pred_labels = self.feature_extractor.label_encoder.inverse_transform(y_pred)\n",
    "        \n",
    "        # Analyze errors\n",
    "        errors = {\n",
    "            'false_positives': [],\n",
    "            'false_negatives': [],\n",
    "            'confusion_pairs': {},\n",
    "            'entity_errors': {}\n",
    "        }\n",
    "        \n",
    "        token_idx = 0\n",
    "        for sample_idx, (tokens, true_labels) in enumerate(zip(X_tokens, y_labels)):\n",
    "            for token_pos, (token, true_label) in enumerate(zip(tokens, true_labels)):\n",
    "                if token_idx >= len(y_pred_labels):\n",
    "                    break\n",
    "                    \n",
    "                pred_label = y_pred_labels[token_idx]\n",
    "                \n",
    "                if true_label != pred_label:\n",
    "                    error_info = {\n",
    "                        'sample_idx': sample_idx,\n",
    "                        'token': token,\n",
    "                        'position': token_pos,\n",
    "                        'true_label': true_label,\n",
    "                        'pred_label': pred_label,\n",
    "                        'context': ' '.join(tokens[max(0, token_pos-2):token_pos+3])\n",
    "                    }\n",
    "                    \n",
    "                    # Classify error type\n",
    "                    if true_label == 'O' and pred_label != 'O':\n",
    "                        errors['false_positives'].append(error_info)\n",
    "                    elif true_label != 'O' and pred_label == 'O':\n",
    "                        errors['false_negatives'].append(error_info)\n",
    "                    \n",
    "                    # Track confusion pairs\n",
    "                    pair = (true_label, pred_label)\n",
    "                    errors['confusion_pairs'][pair] = errors['confusion_pairs'].get(pair, 0) + 1\n",
    "                    \n",
    "                    # Track by entity type\n",
    "                    entity_type = true_label.split('-')[-1] if '-' in true_label else true_label\n",
    "                    if entity_type not in errors['entity_errors']:\n",
    "                        errors['entity_errors'][entity_type] = []\n",
    "                    errors['entity_errors'][entity_type].append(error_info)\n",
    "                \n",
    "                token_idx += 1\n",
    "        \n",
    "        self.error_analysis = errors\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nüìä Error Analysis Summary:\")\n",
    "        print(f\"   - False Positives: {len(errors['false_positives'])}\")\n",
    "        print(f\"   - False Negatives: {len(errors['false_negatives'])}\")\n",
    "        print(f\"   - Most Confused Pairs: {sorted(errors['confusion_pairs'].items(), key=lambda x: x[1], reverse=True)[:5]}\")\n",
    "        print(f\"   - Entities with Most Errors: {sorted([(k, len(v)) for k, v in errors['entity_errors'].items()], key=lambda x: x[1], reverse=True)[:5]}\")\n",
    "        \n",
    "        return errors\n",
    "    \n",
    "    def plot_error_analysis(self):\n",
    "        \"\"\"Plot error analysis results\"\"\"\n",
    "        if not self.error_analysis:\n",
    "            print(\"No error analysis data available\")\n",
    "            return\n",
    "        \n",
    "        errors = self.error_analysis\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Error type distribution\n",
    "        plt.subplot(2, 3, 1)\n",
    "        error_types = ['False Positives', 'False Negatives']\n",
    "        error_counts = [len(errors['false_positives']), len(errors['false_negatives'])]\n",
    "        plt.bar(error_types, error_counts, color=['red', 'orange'], alpha=0.7)\n",
    "        plt.title('Error Type Distribution')\n",
    "        plt.ylabel('Count')\n",
    "        for i, v in enumerate(error_counts):\n",
    "            plt.text(i, v + 0.5, str(v), ha='center', va='bottom')\n",
    "        \n",
    "        # Most confused label pairs\n",
    "        plt.subplot(2, 3, 2)\n",
    "        top_confusions = sorted(errors['confusion_pairs'].items(), key=lambda x: x[1], reverse=True)[:8]\n",
    "        if top_confusions:\n",
    "            labels = [f\"{pair[0]}‚Üí{pair[1]}\" for pair, count in top_confusions]\n",
    "            counts = [count for pair, count in top_confusions]\n",
    "            plt.barh(labels, counts, alpha=0.7)\n",
    "            plt.title('Most Confused Label Pairs')\n",
    "            plt.xlabel('Count')\n",
    "        \n",
    "        # Errors by entity type\n",
    "        plt.subplot(2, 3, 3)\n",
    "        entity_error_counts = [(entity, len(error_list)) for entity, error_list in errors['entity_errors'].items()]\n",
    "        entity_error_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        if entity_error_counts:\n",
    "            entities = [item[0] for item in entity_error_counts[:8]]\n",
    "            counts = [item[1] for item in entity_error_counts[:8]]\n",
    "            plt.barh(entities, counts, alpha=0.7, color='lightcoral')\n",
    "            plt.title('Errors by Entity Type')\n",
    "            plt.xlabel('Error Count')\n",
    "        \n",
    "        # False positive analysis\n",
    "        plt.subplot(2, 3, 4)\n",
    "        if errors['false_positives']:\n",
    "            fp_entities = [error['pred_label'].split('-')[-1] if '-' in error['pred_label'] else error['pred_label'] \n",
    "                          for error in errors['false_positives']]\n",
    "            fp_counts = pd.Series(fp_entities).value_counts().head(8)\n",
    "            fp_counts.plot(kind='bar')\n",
    "            plt.title('False Positives by Predicted Entity')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.ylabel('Count')\n",
    "        \n",
    "        # False negative analysis\n",
    "        plt.subplot(2, 3, 5)\n",
    "        if errors['false_negatives']:\n",
    "            fn_entities = [error['true_label'].split('-')[-1] if '-' in error['true_label'] else error['true_label'] \n",
    "                          for error in errors['false_negatives']]\n",
    "            fn_counts = pd.Series(fn_entities).value_counts().head(8)\n",
    "            fn_counts.plot(kind='bar', color='orange')\n",
    "            plt.title('False Negatives by True Entity')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.ylabel('Count')\n",
    "        \n",
    "        # Error pattern summary\n",
    "        plt.subplot(2, 3, 6)\n",
    "        total_errors = len(errors['false_positives']) + len(errors['false_negatives'])\n",
    "        if total_errors > 0:\n",
    "            fp_rate = len(errors['false_positives']) / total_errors\n",
    "            fn_rate = len(errors['false_negatives']) / total_errors\n",
    "            \n",
    "            plt.pie([fp_rate, fn_rate], labels=['False Positives', 'False Negatives'], \n",
    "                   autopct='%1.1f%%', colors=['red', 'orange'])\n",
    "            plt.title('Error Distribution')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print detailed examples\n",
    "        self._print_error_examples()\n",
    "    \n",
    "    def _print_error_examples(self):\n",
    "        \"\"\"Print examples of different error types\"\"\"\n",
    "        errors = self.error_analysis\n",
    "        \n",
    "        print(\"\\nüîç Error Examples:\")\n",
    "        \n",
    "        # False positive examples\n",
    "        if errors['false_positives']:\n",
    "            print(f\"\\n‚ùå False Positive Examples (showing first 3):\")\n",
    "            for i, error in enumerate(errors['false_positives'][:3]):\n",
    "                print(f\"   {i+1}. Token: '{error['token']}' | Predicted: {error['pred_label']} | True: {error['true_label']}\")\n",
    "                print(f\"      Context: {error['context']}\")\n",
    "        \n",
    "        # False negative examples\n",
    "        if errors['false_negatives']:\n",
    "            print(f\"\\n‚ùå False Negative Examples (showing first 3):\")\n",
    "            for i, error in enumerate(errors['false_negatives'][:3]):\n",
    "                print(f\"   {i+1}. Token: '{error['token']}' | Predicted: {error['pred_label']} | True: {error['true_label']}\")\n",
    "                print(f\"      Context: {error['context']}\")\n",
    "\n",
    "# Load test data for error analysis\n",
    "with open('../data/augmented_dataset.json', 'r', encoding='utf-8') as f:\n",
    "    test_dataset = json.load(f)\n",
    "\n",
    "# Run error analysis\n",
    "error_analyzer = ErrorAnalyzer(best_model, feature_extractor, tokenizer)\n",
    "error_results = error_analyzer.analyze_predictions(test_dataset, sample_size=100)\n",
    "error_analyzer.plot_error_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Post-Processing & Data Normalization Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostProcessingValidator:\n",
    "    \"\"\"Validate and demonstrate post-processing capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.normalization_rules = {\n",
    "            'date_formats': ['%d/%m/%Y', '%d-%m-%Y', '%Y-%m-%d', '%d %b %Y'],\n",
    "            'currency_symbols': ['Rp', 'IDR', 'Rupiah'],\n",
    "            'npwp_pattern': r'\\d{2}\\.\\d{3}\\.\\d{3}\\.\\d{1}\\-\\d{3}\\.\\d{3}'\n",
    "        }\n",
    "    \n",
    "    def normalize_date(self, date_str: str) -> str:\n",
    "        \"\"\"Normalize date to ISO format (YYYY-MM-DD)\"\"\"\n",
    "        if not date_str or date_str.strip() == '':\n",
    "            return ''\n",
    "        \n",
    "        # Common Indonesian date patterns\n",
    "        patterns = [\n",
    "            (r'(\\d{1,2})[/\\-](\\d{1,2})[/\\-](\\d{4})', '%d/%m/%Y'),  # DD/MM/YYYY or DD-MM-YYYY\n",
    "            (r'(\\d{4})[/\\-](\\d{1,2})[/\\-](\\d{1,2})', '%Y/%m/%d'),  # YYYY/MM/DD or YYYY-MM-DD\n",
    "            (r'(\\d{1,2})\\s+(\\w+)\\s+(\\d{4})', '%d %B %Y'),  # DD Month YYYY\n",
    "        ]\n",
    "        \n",
    "        for pattern, format_str in patterns:\n",
    "            match = re.search(pattern, date_str)\n",
    "            if match:\n",
    "                try:\n",
    "                    if format_str == '%d/%m/%Y':\n",
    "                        day, month, year = match.groups()\n",
    "                        return f\"{year}-{month.zfill(2)}-{day.zfill(2)}\"\n",
    "                    elif format_str == '%Y/%m/%d':\n",
    "                        year, month, day = match.groups()\n",
    "                        return f\"{year}-{month.zfill(2)}-{day.zfill(2)}\"\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        return date_str  # Return original if no pattern matches\n",
    "    \n",
    "    def normalize_currency(self, amount_str: str) -> str:\n",
    "        \"\"\"Normalize currency amount (remove symbols, standardize format)\"\"\"\n",
    "        if not amount_str or amount_str.strip() == '':\n",
    "            return ''\n",
    "        \n",
    "        # Remove currency symbols\n",
    "        cleaned = amount_str\n",
    "        for symbol in self.normalization_rules['currency_symbols']:\n",
    "            cleaned = cleaned.replace(symbol, '').replace('.', '').strip()\n",
    "        \n",
    "        # Extract numbers and commas\n",
    "        numbers = re.findall(r'[\\d,]+', cleaned)\n",
    "        if numbers:\n",
    "            # Remove commas and return clean number\n",
    "            return numbers[0].replace(',', '')\n",
    "        \n",
    "        return amount_str\n",
    "    \n",
    "    def normalize_npwp(self, npwp_str: str) -> str:\n",
    "        \"\"\"Normalize NPWP format\"\"\"\n",
    "        if not npwp_str or npwp_str.strip() == '':\n",
    "            return ''\n",
    "        \n",
    "        # Extract numbers only then reformat\n",
    "        numbers = re.sub(r'[^\\d]', '', npwp_str)\n",
    "        \n",
    "        if len(numbers) == 15:  # Standard NPWP length\n",
    "            # Format: XX.XXX.XXX.X-XXX.XXX\n",
    "            return f\"{numbers[:2]}.{numbers[2:5]}.{numbers[5:8]}.{numbers[8]}-{numbers[9:12]}.{numbers[12:15]}\"\n",
    "        \n",
    "        return npwp_str\n",
    "    \n",
    "    def validate_post_processing(self, sample_extractions: List[Dict]) -> Dict:\n",
    "        \"\"\"Validate post-processing on sample extractions\"\"\"\n",
    "        print(\"üîß Validating post-processing and normalization...\")\n",
    "        \n",
    "        validation_results = {\n",
    "            'date_normalizations': [],\n",
    "            'currency_normalizations': [],\n",
    "            'npwp_normalizations': [],\n",
    "            'success_rate': 0\n",
    "        }\n",
    "        \n",
    "        successful_normalizations = 0\n",
    "        total_normalizations = 0\n",
    "        \n",
    "        for sample in sample_extractions:\n",
    "            # Test date normalization\n",
    "            if 'invoice_date' in sample and sample['invoice_date']:\n",
    "                original = sample['invoice_date']\n",
    "                normalized = self.normalize_date(original)\n",
    "                validation_results['date_normalizations'].append({\n",
    "                    'original': original,\n",
    "                    'normalized': normalized,\n",
    "                    'improved': normalized != original and re.match(r'\\d{4}-\\d{2}-\\d{2}', normalized)\n",
    "                })\n",
    "                total_normalizations += 1\n",
    "                if normalized != original:\n",
    "                    successful_normalizations += 1\n",
    "            \n",
    "            # Test currency normalization\n",
    "            if 'invoice_total' in sample and sample['invoice_total']:\n",
    "                original = sample['invoice_total']\n",
    "                normalized = self.normalize_currency(original)\n",
    "                validation_results['currency_normalizations'].append({\n",
    "                    'original': original,\n",
    "                    'normalized': normalized,\n",
    "                    'improved': normalized != original and normalized.isdigit()\n",
    "                })\n",
    "                total_normalizations += 1\n",
    "                if normalized != original:\n",
    "                    successful_normalizations += 1\n",
    "            \n",
    "            # Test NPWP normalization\n",
    "            if 'vendor' in sample and 'tax_id' in sample['vendor'] and sample['vendor']['tax_id']:\n",
    "                original = sample['vendor']['tax_id']\n",
    "                normalized = self.normalize_npwp(original)\n",
    "                validation_results['npwp_normalizations'].append({\n",
    "                    'original': original,\n",
    "                    'normalized': normalized,\n",
    "                    'improved': normalized != original and re.match(self.normalization_rules['npwp_pattern'], normalized)\n",
    "                })\n",
    "                total_normalizations += 1\n",
    "                if normalized != original:\n",
    "                    successful_normalizations += 1\n",
    "        \n",
    "        validation_results['success_rate'] = successful_normalizations / total_normalizations if total_normalizations > 0 else 0\n",
    "        \n",
    "        print(f\"‚úÖ Post-processing validation completed:\")\n",
    "        print(f\"   - Date normalizations: {len(validation_results['date_normalizations'])}\")\n",
    "        print(f\"   - Currency normalizations: {len(validation_results['currency_normalizations'])}\")\n",
    "        print(f\"   - NPWP normalizations: {len(validation_results['npwp_normalizations'])}\")\n",
    "        print(f\"   - Overall success rate: {validation_results['success_rate']:.2%}\")\n",
    "        \n",
    "        return validation_results\n",
    "    \n",
    "    def demonstrate_normalization(self, validation_results: Dict):\n",
    "        \"\"\"Demonstrate normalization examples\"\"\"\n",
    "        print(\"\\nüìã Normalization Examples:\")\n",
    "        \n",
    "        # Show date normalization examples\n",
    "        if validation_results['date_normalizations']:\n",
    "            print(\"\\nüìÖ Date Normalization Examples:\")\n",
    "            for i, example in enumerate(validation_results['date_normalizations'][:3]):\n",
    "                status = \"‚úÖ\" if example['improved'] else \"‚û°Ô∏è\"\n",
    "                print(f\"   {i+1}. {status} '{example['original']}' ‚Üí '{example['normalized']}'\")\n",
    "        \n",
    "        # Show currency normalization examples\n",
    "        if validation_results['currency_normalizations']:\n",
    "            print(\"\\nüí∞ Currency Normalization Examples:\")\n",
    "            for i, example in enumerate(validation_results['currency_normalizations'][:3]):\n",
    "                status = \"‚úÖ\" if example['improved'] else \"‚û°Ô∏è\"\n",
    "                print(f\"   {i+1}. {status} '{example['original']}' ‚Üí '{example['normalized']}'\")\n",
    "        \n",
    "        # Show NPWP normalization examples\n",
    "        if validation_results['npwp_normalizations']:\n",
    "            print(\"\\nüè¢ NPWP Normalization Examples:\")\n",
    "            for i, example in enumerate(validation_results['npwp_normalizations'][:3]):\n",
    "                status = \"‚úÖ\" if example['improved'] else \"‚û°Ô∏è\"\n",
    "                print(f\"   {i+1}. {status} '{example['original']}' ‚Üí '{example['normalized']}'\")\n",
    "\n",
    "# Create sample extractions for validation\n",
    "sample_extractions = [\n",
    "    {\n",
    "        'invoice_date': '15/03/2024',\n",
    "        'invoice_total': 'Rp 500,000',\n",
    "        'vendor': {'tax_id': '01000000100000'}\n",
    "    },\n",
    "    {\n",
    "        'invoice_date': '20-04-2024',\n",
    "        'invoice_total': 'IDR 1.250.000',\n",
    "        'vendor': {'tax_id': '02.345.678.9-012.345'}\n",
    "    },\n",
    "    {\n",
    "        'invoice_date': '2024/05/10',\n",
    "        'invoice_total': '750000',\n",
    "        'vendor': {'tax_id': '03.456.789.0-123.456'}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run post-processing validation\n",
    "post_processor = PostProcessingValidator()\n",
    "validation_results = post_processor.validate_post_processing(sample_extractions)\n",
    "post_processor.demonstrate_normalization(validation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Evaluation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive evaluation report\n",
    "evaluation_report = {\n",
    "    'model_performance': {\n",
    "        'best_model': training_summary['best_model']['name'],\n",
    "        'best_f1_score': training_summary['best_model']['f1_score'],\n",
    "        'target_achieved': training_summary['target_achieved'],\n",
    "        'total_training_iterations': training_summary['total_iterations']\n",
    "    },\n",
    "    'cross_validation': cv_summary if cv_summary else None,\n",
    "    'error_analysis': {\n",
    "        'total_errors': len(error_results['false_positives']) + len(error_results['false_negatives']),\n",
    "        'false_positive_rate': len(error_results['false_positives']) / (len(error_results['false_positives']) + len(error_results['false_negatives'])) if (len(error_results['false_positives']) + len(error_results['false_negatives'])) > 0 else 0,\n",
    "        'most_problematic_entities': sorted([(k, len(v)) for k, v in error_results['entity_errors'].items()], key=lambda x: x[1], reverse=True)[:5]\n",
    "    },\n",
    "    'post_processing': {\n",
    "        'normalization_success_rate': validation_results['success_rate'],\n",
    "        'date_normalization_available': len(validation_results['date_normalizations']) > 0,\n",
    "        'currency_normalization_available': len(validation_results['currency_normalizations']) > 0,\n",
    "        'npwp_normalization_available': len(validation_results['npwp_normalizations']) > 0\n",
    "    },\n",
    "    'production_readiness': {\n",
    "        'f1_score_target_met': training_summary['best_model']['f1_score'] > 0.9,\n",
    "        'minimum_iterations_completed': training_summary['total_iterations'] >= 20,\n",
    "        'cross_validation_stable': cv_summary['cv_stable'] if cv_summary else False,\n",
    "        'error_analysis_completed': True,\n",
    "        'post_processing_validated': True\n",
    "    },\n",
    "    'recommendations': [],\n",
    "    'evaluation_date': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Generate recommendations based on results\n",
    "recommendations = []\n",
    "\n",
    "if not evaluation_report['production_readiness']['f1_score_target_met']:\n",
    "    recommendations.append(\"Consider additional training data or feature engineering to improve F1 score above 0.9\")\n",
    "\n",
    "if error_results['false_positives']:\n",
    "    recommendations.append(\"Implement stricter post-processing rules to reduce false positives\")\n",
    "\n",
    "if error_results['false_negatives']:\n",
    "    recommendations.append(\"Add more training examples for entities with high false negative rates\")\n",
    "\n",
    "if validation_results['success_rate'] < 0.5:\n",
    "    recommendations.append(\"Improve post-processing normalization rules for better data quality\")\n",
    "\n",
    "if cv_summary and not cv_summary['cv_stable']:\n",
    "    recommendations.append(\"Model shows high variance across folds - consider regularization or more training data\")\n",
    "\n",
    "recommendations.append(\"Consider implementing layout-aware features using actual document coordinates\")\n",
    "recommendations.append(\"Add more Indonesian-specific linguistic features for better entity recognition\")\n",
    "recommendations.append(\"Implement active learning pipeline for continuous model improvement\")\n",
    "\n",
    "evaluation_report['recommendations'] = recommendations\n",
    "\n",
    "# Save evaluation report\n",
    "with open('../models/evaluation_report.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(evaluation_report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Print final report\n",
    "print(\"üìã FINAL EVALUATION REPORT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nüéØ Model Performance:\")\n",
    "print(f\"   - Best Model: {evaluation_report['model_performance']['best_model']}\")\n",
    "print(f\"   - Best F1 Score: {evaluation_report['model_performance']['best_f1_score']:.4f}\")\n",
    "print(f\"   - Target F1 > 0.9: {'‚úÖ ACHIEVED' if evaluation_report['model_performance']['target_achieved'] else '‚ùå NOT ACHIEVED'}\")\n",
    "print(f\"   - Training Iterations: {evaluation_report['model_performance']['total_training_iterations']}\")\n",
    "\n",
    "if cv_summary:\n",
    "    print(f\"\\nüìä Cross-Validation:\")\n",
    "    print(f\"   - Mean F1 Score: {cv_summary['mean_f1']:.4f} ¬± {cv_summary['std_f1']:.4f}\")\n",
    "    print(f\"   - Stability: {'‚úÖ STABLE' if cv_summary['cv_stable'] else '‚ö†Ô∏è  UNSTABLE'}\")\n",
    "\n",
    "print(f\"\\nüîç Error Analysis:\")\n",
    "print(f\"   - Total Errors: {evaluation_report['error_analysis']['total_errors']}\")\n",
    "print(f\"   - False Positive Rate: {evaluation_report['error_analysis']['false_positive_rate']:.2%}\")\n",
    "print(f\"   - Most Problematic Entities: {evaluation_report['error_analysis']['most_problematic_entities'][:3]}\")\n",
    "\n",
    "print(f\"\\nüîß Post-Processing:\")\n",
    "print(f\"   - Normalization Success Rate: {evaluation_report['post_processing']['normalization_success_rate']:.2%}\")\n",
    "print(f\"   - Date Normalization: {'‚úÖ' if evaluation_report['post_processing']['date_normalization_available'] else '‚ùå'}\")\n",
    "print(f\"   - Currency Normalization: {'‚úÖ' if evaluation_report['post_processing']['currency_normalization_available'] else '‚ùå'}\")\n",
    "print(f\"   - NPWP Normalization: {'‚úÖ' if evaluation_report['post_processing']['npwp_normalization_available'] else '‚ùå'}\")\n",
    "\n",
    "print(f\"\\nüöÄ Production Readiness:\")\n",
    "readiness_items = [\n",
    "    (\"F1 Score Target (>0.9)\", evaluation_report['production_readiness']['f1_score_target_met']),\n",
    "    (\"Minimum Iterations (‚â•20)\", evaluation_report['production_readiness']['minimum_iterations_completed']),\n",
    "    (\"Cross-Validation Stable\", evaluation_report['production_readiness']['cross_validation_stable']),\n",
    "    (\"Error Analysis Complete\", evaluation_report['production_readiness']['error_analysis_completed']),\n",
    "    (\"Post-Processing Validated\", evaluation_report['production_readiness']['post_processing_validated'])\n",
    "]\n",
    "\n",
    "for item, status in readiness_items:\n",
    "    print(f\"   {'‚úÖ' if status else '‚ùå'} {item}\")\n",
    "\n",
    "overall_ready = sum([status for _, status in readiness_items]) >= 4\n",
    "print(f\"\\n{'üéâ PRODUCTION READY' if overall_ready else '‚ö†Ô∏è  NEEDS IMPROVEMENT'}\")\n",
    "\n",
    "print(f\"\\nüí° Recommendations:\")\n",
    "for i, rec in enumerate(evaluation_report['recommendations'], 1):\n",
    "    print(f\"   {i}. {rec}\")\n",
    "\n",
    "print(f\"\\nüìÅ Evaluation report saved to: ../models/evaluation_report.json\")\n",
    "print(f\"‚è∞ Evaluation completed at: {evaluation_report['evaluation_date']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"\n  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}