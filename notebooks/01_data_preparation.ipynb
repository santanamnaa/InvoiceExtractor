{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoice & Tax Document Data Preparation\n",
    "\n",
    "This notebook handles data preparation for training layout-aware models for Indonesian invoice extraction.\n",
    "\n",
    "## Architecture Overview\n",
    "- **Hybrid Regex+NER**: Use regex for fixed patterns (NPWP, dates, currency) and NER for flexible entities (names, addresses)\n",
    "- **Layout-aware**: Leverage bounding box information for better extraction\n",
    "- **IOB2 Format**: Consistent labeling format (B-INVOICE_DATE, I-INVOICE_DATE, etc.)\n",
    "- **Data Augmentation**: Generate variations for robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path to import our extraction service\n",
    "sys.path.append('..')\n",
    "from extraction_service import InvoiceExtractor\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ“ Data preparation notebook initialized\")\n",
    "print(f\"âœ“ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection & Annotation Framework\n",
    "\n",
    "### IOB2 Labeling Schema\n",
    "We'll use consistent IOB2 format for all entities:\n",
    "\n",
    "- **B-INVOICE_NUMBER**: Beginning of invoice number\n",
    "- **I-INVOICE_NUMBER**: Inside invoice number\n",
    "- **B-INVOICE_DATE**: Beginning of invoice date\n",
    "- **B-VENDOR_NAME**: Beginning of vendor name\n",
    "- **B-BUYER_NAME**: Beginning of buyer name\n",
    "- **B-AMOUNT**: Beginning of amount\n",
    "- **B-TAX_ID**: Beginning of tax ID (NPWP)\n",
    "- **O**: Outside (not an entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentAnnotator:\n",
    "    \"\"\"Handle document annotation and IOB2 format conversion\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.entity_types = [\n",
    "            'INVOICE_NUMBER', 'INVOICE_DATE', 'DUE_DATE', 'BILLING_MONTH',\n",
    "            'VENDOR_NAME', 'VENDOR_TAX_ID', 'VENDOR_ADDRESS',\n",
    "            'BUYER_NAME', 'BUYER_TAX_ID', 'BUYER_ADDRESS',\n",
    "            'AMOUNT', 'SUBTOTAL', 'TAX_AMOUNT', 'TAX_PERCENTAGE',\n",
    "            'BANK_NAME', 'ACCOUNT_NUMBER', 'VIRTUAL_ACCOUNT',\n",
    "            'FAKTUR_NUMBER', 'SIGNER_NAME', 'SIGNER_POSITION'\n",
    "        ]\n",
    "        \n",
    "        self.regex_patterns = {\n",
    "            'INVOICE_NUMBER': r'(?:invoice|faktur|no\\.?\\s*invoice|nomor\\s*faktur)[\\s:]*([A-Z0-9\\-/]+)',\n",
    "            'INVOICE_DATE': r'(?:tanggal|date|invoice\\s*date)[\\s:]*(\\d{1,2}[\\/\\-]\\d{1,2}[\\/\\-]\\d{2,4})',\n",
    "            'TAX_ID': r'(?:npwp)[\\s:]*([0-9\\.\\-]{15,20})',\n",
    "            'AMOUNT': r'(?:total|jumlah|amount)[\\s:]*(?:rp\\.?\\s*)?([0-9,\\.]+)',\n",
    "            'BANK_NAME': r'(?:bank|nama\\s*bank)[\\s:]*([A-Z\\s]+)',\n",
    "        }\n",
    "    \n",
    "    def create_sample_annotations(self, num_samples: int = 50) -> List[Dict]:\n",
    "        \"\"\"Create sample annotated data for training\"\"\"\n",
    "        samples = []\n",
    "        \n",
    "        # Sample invoice texts with annotations\n",
    "        sample_texts = [\n",
    "            {\n",
    "                'text': 'INVOICE NO: INV-2024-001 DATE: 15/03/2024 PT TELKOM INDONESIA NPWP: 01.000.000.1-000.000 TOTAL: 500,000',\n",
    "                'entities': [\n",
    "                    {'start': 12, 'end': 24, 'label': 'INVOICE_NUMBER'},\n",
    "                    {'start': 31, 'end': 41, 'label': 'INVOICE_DATE'},\n",
    "                    {'start': 42, 'end': 60, 'label': 'VENDOR_NAME'},\n",
    "                    {'start': 67, 'end': 85, 'label': 'VENDOR_TAX_ID'},\n",
    "                    {'start': 93, 'end': 100, 'label': 'AMOUNT'}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                'text': 'FAKTUR PAJAK: 010.000-24.00000001 TANGGAL: 20/03/2024 CV MAJU JAYA ALAMAT: JL SUDIRMAN NO 123 JAKARTA',\n",
    "                'entities': [\n",
    "                    {'start': 14, 'end': 32, 'label': 'FAKTUR_NUMBER'},\n",
    "                    {'start': 42, 'end': 52, 'label': 'INVOICE_DATE'},\n",
    "                    {'start': 53, 'end': 65, 'label': 'BUYER_NAME'},\n",
    "                    {'start': 74, 'end': 101, 'label': 'BUYER_ADDRESS'}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Generate variations\n",
    "        for i in range(num_samples):\n",
    "            base_sample = sample_texts[i % len(sample_texts)]\n",
    "            \n",
    "            # Add variations by changing numbers/dates\n",
    "            text_variations = self._generate_text_variations(base_sample['text'])\n",
    "            \n",
    "            sample = {\n",
    "                'id': f'sample_{i:03d}',\n",
    "                'text': text_variations,\n",
    "                'entities': base_sample['entities'],\n",
    "                'tokens': text_variations.split(),\n",
    "                'labels': self._convert_to_iob2(text_variations, base_sample['entities'])\n",
    "            }\n",
    "            samples.append(sample)\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def _generate_text_variations(self, text: str) -> str:\n",
    "        \"\"\"Generate variations of invoice text for data augmentation\"\"\"\n",
    "        # Vary numbers\n",
    "        text = re.sub(r'\\d{3,}', lambda m: str(int(m.group()) + np.random.randint(-1000, 1000)), text)\n",
    "        \n",
    "        # Vary dates\n",
    "        text = re.sub(r'\\d{1,2}/\\d{1,2}/\\d{4}', \n",
    "                     lambda m: f\"{np.random.randint(1,29):02d}/{np.random.randint(1,13):02d}/2024\", text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _convert_to_iob2(self, text: str, entities: List[Dict]) -> List[str]:\n",
    "        \"\"\"Convert entity annotations to IOB2 format\"\"\"\n",
    "        tokens = text.split()\n",
    "        labels = ['O'] * len(tokens)\n",
    "        \n",
    "        # Calculate token positions\n",
    "        token_positions = []\n",
    "        current_pos = 0\n",
    "        for token in tokens:\n",
    "            start = text.find(token, current_pos)\n",
    "            end = start + len(token)\n",
    "            token_positions.append((start, end))\n",
    "            current_pos = end\n",
    "        \n",
    "        # Assign IOB2 labels\n",
    "        for entity in entities:\n",
    "            entity_start, entity_end = entity['start'], entity['end']\n",
    "            entity_label = entity['label']\n",
    "            \n",
    "            first_token = True\n",
    "            for i, (token_start, token_end) in enumerate(token_positions):\n",
    "                # Check if token overlaps with entity\n",
    "                if token_start < entity_end and token_end > entity_start:\n",
    "                    if first_token:\n",
    "                        labels[i] = f'B-{entity_label}'\n",
    "                        first_token = False\n",
    "                    else:\n",
    "                        labels[i] = f'I-{entity_label}'\n",
    "        \n",
    "        return labels\n",
    "\n",
    "# Initialize annotator and create sample data\n",
    "annotator = DocumentAnnotator()\n",
    "sample_data = annotator.create_sample_annotations(100)\n",
    "\n",
    "print(f\"âœ“ Created {len(sample_data)} annotated samples\")\n",
    "print(f\"âœ“ Entity types: {len(annotator.entity_types)}\")\n",
    "print(f\"âœ“ Sample data structure: {list(sample_data[0].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Augmentation Pipeline\n",
    "\n",
    "Generate variations in invoice format, fonts, layouts, and noise to make the model robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAugmentation:\n",
    "    \"\"\"Handle data augmentation for invoice documents\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.currency_variations = ['Rp', 'IDR', 'Rupiah']\n",
    "        self.date_formats = ['%d/%m/%Y', '%d-%m-%Y', '%Y-%m-%d', '%d %b %Y']\n",
    "        self.company_prefixes = ['PT', 'CV', 'UD', 'Perusahaan']\n",
    "        \n",
    "    def augment_dataset(self, data: List[Dict], multiplier: int = 3) -> List[Dict]:\n",
    "        \"\"\"Augment dataset by creating variations\"\"\"\n",
    "        augmented_data = data.copy()\n",
    "        \n",
    "        for original in data:\n",
    "            for i in range(multiplier - 1):  # -1 because we already have original\n",
    "                augmented = self._create_variation(original, i)\n",
    "                augmented_data.append(augmented)\n",
    "        \n",
    "        return augmented_data\n",
    "    \n",
    "    def _create_variation(self, original: Dict, variation_id: int) -> Dict:\n",
    "        \"\"\"Create a single variation of the original data\"\"\"\n",
    "        text = original['text']\n",
    "        \n",
    "        # Text variations\n",
    "        text = self._vary_currency_format(text)\n",
    "        text = self._vary_date_format(text)\n",
    "        text = self._vary_company_names(text)\n",
    "        text = self._add_noise(text)\n",
    "        \n",
    "        # Update entities positions (simplified - in real scenario would need proper alignment)\n",
    "        entities = original['entities'].copy()\n",
    "        \n",
    "        variation = {\n",
    "            'id': f\"{original['id']}_var_{variation_id}\",\n",
    "            'text': text,\n",
    "            'entities': entities,\n",
    "            'tokens': text.split(),\n",
    "            'labels': annotator._convert_to_iob2(text, entities)\n",
    "        }\n",
    "        \n",
    "        return variation\n",
    "    \n",
    "    def _vary_currency_format(self, text: str) -> str:\n",
    "        \"\"\"Vary currency format representation\"\"\"\n",
    "        currency = np.random.choice(self.currency_variations)\n",
    "        text = re.sub(r'\\b(?:Rp|IDR|Rupiah)\\b', currency, text, flags=re.IGNORECASE)\n",
    "        return text\n",
    "    \n",
    "    def _vary_date_format(self, text: str) -> str:\n",
    "        \"\"\"Vary date format representation\"\"\"\n",
    "        # This is simplified - real implementation would parse and reformat dates\n",
    "        return text\n",
    "    \n",
    "    def _vary_company_names(self, text: str) -> str:\n",
    "        \"\"\"Vary company name prefixes\"\"\"\n",
    "        for prefix in ['PT', 'CV', 'UD']:\n",
    "            if prefix in text:\n",
    "                new_prefix = np.random.choice(self.company_prefixes)\n",
    "                text = text.replace(prefix, new_prefix, 1)\n",
    "                break\n",
    "        return text\n",
    "    \n",
    "    def _add_noise(self, text: str) -> str:\n",
    "        \"\"\"Add OCR-like noise to text\"\"\"\n",
    "        # Add occasional OCR errors\n",
    "        if np.random.random() < 0.1:  # 10% chance\n",
    "            # Replace similar looking characters\n",
    "            replacements = {'0': 'O', '1': 'l', '5': 'S', '8': 'B'}\n",
    "            for old, new in replacements.items():\n",
    "                if old in text and np.random.random() < 0.3:\n",
    "                    text = text.replace(old, new, 1)\n",
    "        \n",
    "        return text\n",
    "\n",
    "# Apply data augmentation\n",
    "augmenter = DataAugmentation()\n",
    "augmented_data = augmenter.augment_dataset(sample_data, multiplier=3)\n",
    "\n",
    "print(f\"âœ“ Original dataset size: {len(sample_data)}\")\n",
    "print(f\"âœ“ Augmented dataset size: {len(augmented_data)}\")\n",
    "print(f\"âœ“ Augmentation multiplier: 3x\")\n",
    "\n",
    "# Display sample augmented data\n",
    "print(\"\\n--- Sample Original vs Augmented ---\")\n",
    "print(f\"Original: {sample_data[0]['text'][:100]}...\")\n",
    "print(f\"Augmented: {augmented_data[1]['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cross-Validation Setup\n",
    "\n",
    "Implement K-fold cross-validation for stable performance estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossValidationSetup:\n",
    "    \"\"\"Handle cross-validation for invoice extraction models\"\"\"\n",
    "    \n",
    "    def __init__(self, k_folds: int = 5, random_state: int = 42):\n",
    "        self.k_folds = k_folds\n",
    "        self.random_state = random_state\n",
    "        self.kf = KFold(n_splits=k_folds, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    def create_folds(self, data: List[Dict]) -> List[Tuple[List[Dict], List[Dict]]]:\n",
    "        \"\"\"Create k-fold cross-validation splits\"\"\"\n",
    "        data_array = np.array(data)\n",
    "        folds = []\n",
    "        \n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(self.kf.split(data)):\n",
    "            train_data = data_array[train_idx].tolist()\n",
    "            val_data = data_array[val_idx].tolist()\n",
    "            folds.append((train_data, val_data))\n",
    "            \n",
    "            print(f\"Fold {fold_idx + 1}: Train={len(train_data)}, Val={len(val_data)}\")\n",
    "        \n",
    "        return folds\n",
    "    \n",
    "    def analyze_label_distribution(self, data: List[Dict]) -> Dict[str, int]:\n",
    "        \"\"\"Analyze distribution of entity labels\"\"\"\n",
    "        label_counts = {}\n",
    "        \n",
    "        for sample in data:\n",
    "            for label in sample['labels']:\n",
    "                label_counts[label] = label_counts.get(label, 0) + 1\n",
    "        \n",
    "        return label_counts\n",
    "    \n",
    "    def plot_label_distribution(self, data: List[Dict]):\n",
    "        \"\"\"Plot label distribution\"\"\"\n",
    "        label_counts = self.analyze_label_distribution(data)\n",
    "        \n",
    "        # Separate entity labels from 'O'\n",
    "        entity_labels = {k: v for k, v in label_counts.items() if k != 'O'}\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Plot entity labels\n",
    "        plt.subplot(1, 2, 1)\n",
    "        labels, counts = zip(*entity_labels.items())\n",
    "        plt.bar(range(len(labels)), counts)\n",
    "        plt.xticks(range(len(labels)), labels, rotation=45, ha='right')\n",
    "        plt.title('Entity Label Distribution')\n",
    "        plt.ylabel('Count')\n",
    "        \n",
    "        # Plot O vs entities ratio\n",
    "        plt.subplot(1, 2, 2)\n",
    "        o_count = label_counts.get('O', 0)\n",
    "        entity_count = sum(entity_labels.values())\n",
    "        plt.pie([o_count, entity_count], labels=['O (Outside)', 'Entities'], autopct='%1.1f%%')\n",
    "        plt.title('O vs Entity Ratio')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return label_counts\n",
    "\n",
    "# Setup cross-validation\n",
    "cv_setup = CrossValidationSetup(k_folds=5)\n",
    "cv_folds = cv_setup.create_folds(augmented_data)\n",
    "\n",
    "print(f\"\\nâœ“ Created {len(cv_folds)} cross-validation folds\")\n",
    "print(f\"âœ“ K-fold configuration: {cv_setup.k_folds} folds\")\n",
    "\n",
    "# Analyze label distribution\n",
    "label_distribution = cv_setup.analyze_label_distribution(augmented_data)\n",
    "print(f\"\\nâœ“ Found {len(label_distribution)} unique labels\")\n",
    "print(f\"âœ“ Most frequent labels: {sorted(label_distribution.items(), key=lambda x: x[1], reverse=True)[:5]}\")\n",
    "\n",
    "# Plot distribution\n",
    "cv_setup.plot_label_distribution(augmented_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Prepared Data\n",
    "\n",
    "Save the prepared and augmented dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "# Save processed data\n",
    "with open('../data/augmented_dataset.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(augmented_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Save cross-validation folds\n",
    "for i, (train_data, val_data) in enumerate(cv_folds):\n",
    "    with open(f'../data/fold_{i+1}_train.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    with open(f'../data/fold_{i+1}_val.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(val_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Save label distribution\n",
    "with open('../data/label_distribution.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(label_distribution, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Create training metadata\n",
    "metadata = {\n",
    "    'dataset_size': len(augmented_data),\n",
    "    'original_size': len(sample_data),\n",
    "    'augmentation_multiplier': 3,\n",
    "    'k_folds': cv_setup.k_folds,\n",
    "    'entity_types': annotator.entity_types,\n",
    "    'unique_labels': list(label_distribution.keys()),\n",
    "    'created_at': datetime.now().isoformat(),\n",
    "    'data_format': 'IOB2'\n",
    "}\n",
    "\n",
    "with open('../data/metadata.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"âœ“ Data preparation completed successfully!\")\n",
    "print(f\"âœ“ Saved {len(augmented_data)} samples to '../data/augmented_dataset.json'\")\n",
    "print(f\"âœ“ Saved {len(cv_folds)} cross-validation folds\")\n",
    "print(f\"âœ“ Saved metadata and label distribution\")\n",
    "print(f\"\\nðŸ“Š Dataset Summary:\")\n",
    "print(f\"   - Total samples: {metadata['dataset_size']}\")\n",
    "print(f\"   - Entity types: {len(metadata['entity_types'])}\")\n",
    "print(f\"   - Unique labels: {len(metadata['unique_labels'])}\")\n",
    "print(f\"   - Cross-validation folds: {metadata['k_folds']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}